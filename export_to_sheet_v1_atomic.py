# export_to_sheet_v1_atomic.py
# Version 1.0: Context Engineering - Atomic Prompting Integration
# ================================================================
"""
Chat Logger v1.0 with Atomic Prompting Principles
==================================================

‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Atomic Prompting ‡∏à‡∏≤‡∏Å Context Engineering:
1. Token efficiency measurement
2. Atomic prompt analysis
3. Quality vs Token ROI tracking
4. Minimal context enhancement

Based on Context Engineering principles:
- [TASK] + [CONSTRAINTS] + [OUTPUT FORMAT]
- Token budget optimization
- Response quality measurement
"""

import json, os, re, time
import requests
import hashlib
from datetime import datetime
from zoneinfo import ZoneInfo
from typing import Dict, List, Any
from dataclasses import dataclass

# Optional imports with fallbacks
try:
    import tiktoken  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏ö token ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥

    tokenizer = tiktoken.get_encoding("cl100k_base")  # GPT-4 tokenizer
except ImportError:
    tokenizer = None
    print("Warning: tiktoken not available, using approximation")

try:
    from dotenv import load_dotenv

    load_dotenv()
except ImportError:
    print("Warning: python-dotenv not available, using environment variables directly")

    def load_dotenv():
        pass

    load_dotenv()

GS_WEBAPP_URL = os.getenv(
    "GS_WEBAPP_URL", "https://script.google.com/macros/s/XXXX/exec"
)
GS_API_KEY = os.getenv("GS_API_KEY", "my-secret-12345")
TZ = os.getenv("LOCAL_TZ", "Asia/Bangkok")
CACHE_FILE = os.getenv("CACHE_FILE", ".export_cache.json")
METRICS_FILE = os.getenv("METRICS_FILE", ".prompt_metrics.json")


@dataclass
class PromptMetrics:
    """Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö metrics ‡∏Ç‡∏≠‡∏á prompt"""

    prompt_text: str  # ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° prompt (‡πÄ‡∏Å‡πá‡∏ö 100 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÅ‡∏£‡∏Å)
    token_count: int  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô token ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ (‡∏ß‡∏±‡∏î‡∏î‡πâ‡∏ß‡∏¢ tiktoken)
    response_length: int  # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á response ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ
    quality_score: float  # ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û (1-5)
    timestamp: str  # ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á metrics
    prompt_type: str  # ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó: "atomic" ‡∏´‡∏£‡∏∑‡∏≠ "enhanced"
    efficiency_ratio: float  # ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô quality/tokens


class AtomicPromptAnalyzer:
    """Analyzer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Atomic Prompting principles"""

    def __init__(self):
        self.metrics_history = []
        self.load_metrics()

    def count_tokens(self, text: str) -> int:
        """‡∏ô‡∏±‡∏ö tokens ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏î‡πâ‡∏ß‡∏¢ tiktoken ‡∏´‡∏£‡∏∑‡∏≠ approximation"""
        if tokenizer:
            return len(tokenizer.encode(text))
        else:
            # Rough approximation: 1 token ‚âà 4 characters
            return max(1, len(text) // 4)

    def analyze_atomic_prompt(self, prompt: str) -> Dict[str, Any]:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå prompt ‡∏ï‡∏≤‡∏° atomic principles"""
        analysis = {
            "token_count": self.count_tokens(prompt),
            "components": self._extract_components(prompt),
            "atomic_score": self._calculate_atomic_score(prompt),
            "optimization_suggestions": self._suggest_optimizations(prompt),
        }
        return analysis

    def _extract_components(self, prompt: str) -> Dict[str, str]:
        """‡πÅ‡∏¢‡∏Å components: TASK + CONSTRAINTS + OUTPUT FORMAT"""
        # Simple heuristic extraction
        lines = prompt.strip().split("\n")
        components = {"task": "", "constraints": "", "output_format": ""}

        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ patterns
        for line in lines:
            line = line.strip()
            if any(
                word in line.lower()
                for word in ["analyze", "extract", "summarize", "classify"]
            ):
                components["task"] = line
            elif any(
                word in line.lower() for word in ["format:", "output:", "return:"]
            ):
                components["output_format"] = line
            elif any(
                word in line.lower() for word in ["only", "must", "should", "limit"]
            ):
                components["constraints"] = line

        return components

    def _calculate_atomic_score(self, prompt: str) -> float:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô atomic principles (0-1)"""
        score = 0.0

        # ‡∏°‡∏µ task ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        if any(
            word in prompt.lower()
            for word in ["analyze", "extract", "summarize", "classify", "identify"]
        ):
            score += 0.4

        # ‡∏°‡∏µ constraints
        if any(
            word in prompt.lower()
            for word in ["only", "must", "should", "limit", "between", "maximum"]
        ):
            score += 0.3

        # ‡∏°‡∏µ output format
        if any(
            word in prompt.lower()
            for word in ["format:", "output:", "return:", "json", "list"]
        ):
            score += 0.3

        return min(1.0, score)

    def _suggest_optimizations(self, prompt: str) -> List[str]:
        """‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á prompt"""
        suggestions = []

        if self.count_tokens(prompt) > 100:
            suggestions.append("‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß prompt - ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô")

        if "please" in prompt.lower() or "could you" in prompt.lower():
            suggestions.append("‡∏•‡∏ö‡∏Ñ‡∏≥‡∏™‡∏∏‡∏†‡∏≤‡∏û - ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ï‡∏£‡∏á‡πÜ")

        if not any(
            word in prompt.lower() for word in ["format:", "output:", "return:"]
        ):
            suggestions.append("‡πÄ‡∏û‡∏¥‡πà‡∏° output format ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô")

        if len(prompt.split("\n")) == 1 and len(prompt) > 50:
            suggestions.append("‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô")

        return suggestions

    def create_atomic_version(self, original_prompt: str) -> str:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á atomic version ‡∏Ç‡∏≠‡∏á prompt"""
        # Extract core task
        core_task = self._extract_core_task(original_prompt)

        # Create atomic structure
        atomic_prompt = f"{core_task}\n\nOutput: JSON format with result only."

        return atomic_prompt

    def _extract_core_task(self, prompt: str) -> str:
        """Extract core task ‡∏à‡∏≤‡∏Å prompt"""
        # ‡πÉ‡∏ä‡πâ regex ‡∏´‡∏≤ action verbs
        action_pattern = (
            r"\b(analyze|extract|summarize|classify|identify|list|find|determine)\b"
        )
        match = re.search(action_pattern, prompt, re.IGNORECASE)

        if match:
            # ‡∏´‡∏≤‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡∏°‡∏µ action verb
            sentences = prompt.split(".")
            for sentence in sentences:
                if match.group().lower() in sentence.lower():
                    return sentence.strip()

        # fallback - ‡πÉ‡∏ä‡πâ‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á prompt
        return prompt.split(".")[0].strip()

    def save_metrics(self, metrics: PromptMetrics):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å metrics"""
        self.metrics_history.append(metrics)
        self._save_metrics_to_file()

    def load_metrics(self):
        """‡πÇ‡∏´‡∏•‡∏î metrics ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå"""
        try:
            if os.path.exists(METRICS_FILE):
                with open(METRICS_FILE, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    # Convert dict back to PromptMetrics
                    self.metrics_history = [PromptMetrics(**item) for item in data]
        except Exception as e:
            print(f"Error loading metrics: {e}")
            self.metrics_history = []

    def _save_metrics_to_file(self):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å metrics ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå"""
        try:
            # Convert PromptMetrics to dict for JSON serialization
            data = [
                {
                    "prompt_text": (
                        m.prompt_text[:100] + "..."
                        if len(m.prompt_text) > 100
                        else m.prompt_text
                    ),
                    "token_count": m.token_count,
                    "response_length": m.response_length,
                    "quality_score": m.quality_score,
                    "timestamp": m.timestamp,
                    "prompt_type": m.prompt_type,
                    "efficiency_ratio": m.efficiency_ratio,
                }
                for m in self.metrics_history
            ]

            with open(METRICS_FILE, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"Error saving metrics: {e}")

    def get_efficiency_report(self) -> Dict[str, Any]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô efficiency"""
        if not self.metrics_history:
            return {"message": "No metrics data available"}

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì averages
        total_tokens = sum(m.token_count for m in self.metrics_history)
        total_quality = sum(m.quality_score for m in self.metrics_history)
        count = len(self.metrics_history)

        atomic_metrics = [m for m in self.metrics_history if m.prompt_type == "atomic"]
        enhanced_metrics = [
            m for m in self.metrics_history if m.prompt_type == "enhanced"
        ]

        report = {
            "total_prompts": count,
            "avg_tokens": total_tokens / count,
            "avg_quality": total_quality / count,
            "avg_efficiency": sum(m.efficiency_ratio for m in self.metrics_history)
            / count,
            "atomic_prompts": len(atomic_metrics),
            "enhanced_prompts": len(enhanced_metrics),
        }

        if atomic_metrics and enhanced_metrics:
            atomic_avg_tokens = sum(m.token_count for m in atomic_metrics) / len(
                atomic_metrics
            )
            enhanced_avg_tokens = sum(m.token_count for m in enhanced_metrics) / len(
                enhanced_metrics
            )

            atomic_avg_quality = sum(m.quality_score for m in atomic_metrics) / len(
                atomic_metrics
            )
            enhanced_avg_quality = sum(m.quality_score for m in enhanced_metrics) / len(
                enhanced_metrics
            )

            report["comparison"] = {
                "token_savings": enhanced_avg_tokens - atomic_avg_tokens,
                "quality_difference": enhanced_avg_quality - atomic_avg_quality,
                "efficiency_improvement": (
                    (atomic_avg_quality / atomic_avg_tokens)
                    / (enhanced_avg_quality / enhanced_avg_tokens)
                    if enhanced_avg_tokens > 0
                    else 0
                ),
            }

        return report


# Initialize analyzer
prompt_analyzer = AtomicPromptAnalyzer()


def log(message):
    print(f"[{datetime.now(ZoneInfo(TZ)).strftime('%Y-%m-%d %H:%M:%S')}] {message}")


def iso(ts):
    if ts is None:
        return datetime.now(ZoneInfo(TZ)).isoformat()
    return datetime.fromtimestamp(float(ts), tz=ZoneInfo(TZ)).isoformat()


def load_cache():
    try:
        if os.path.exists(CACHE_FILE):
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        return {}
    except Exception as e:
        log(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏Ñ‡∏ä: {str(e)}")
        return {}


def save_cache(cache):
    try:
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
    except Exception as e:
        log(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏Ñ‡∏ä: {str(e)}")


def create_row_hash(row):
    key = f"{row['session_id']}_{row['timestamp']}_{row['user_prompt'][:50]}_{row['ai_response'][:50]}"
    return hashlib.md5(key.encode("utf-8")).hexdigest()


def analyze_conversation_with_atomic_prompts(
    user_prompt: str, ai_response: str
) -> Dict[str, Any]:
    """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏î‡πâ‡∏ß‡∏¢ atomic prompting principles"""

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á atomic prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
    atomic_analysis_prompt = f"""Analyze conversation quality:
        User: {user_prompt[:200]}
        AI: {ai_response[:200]}
        Output: sentiment|clarity|usefulness (1-5 each)
    """

    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå atomic prompt
    prompt_analysis = prompt_analyzer.analyze_atomic_prompt(atomic_analysis_prompt)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á enhanced version
    enhanced_prompt = f"""
        Task: Analyze conversation quality and user satisfaction.
        Input:
        - User message: {user_prompt[:200]}
        - AI response: {ai_response[:200]}

        Constraints:
        - Rate each dimension 1-5 only
        - Focus on helpfulness and clarity

        Output format:
        {{"sentiment": X, "clarity": X, "usefulness": X, "overall": X}}
    """

    enhanced_analysis: Dict[str, Any] = prompt_analyzer.analyze_atomic_prompt(
        enhanced_prompt
    )

    # Mock quality assessment (‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM)
    quality_atomic = 3.5  # placeholder
    quality_enhanced = 4.2  # placeholder

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å metrics
    atomic_metrics = PromptMetrics(
        prompt_text=atomic_analysis_prompt,
        token_count=prompt_analysis["token_count"],
        response_length=50,  # mock
        quality_score=quality_atomic,
        timestamp=datetime.now(ZoneInfo(TZ)).isoformat(),
        prompt_type="atomic",
        efficiency_ratio=quality_atomic / prompt_analysis["token_count"],
    )

    enhanced_metrics = PromptMetrics(
        prompt_text=enhanced_prompt,
        token_count=enhanced_analysis["token_count"],
        response_length=80,  # mock
        quality_score=quality_enhanced,
        timestamp=datetime.now(ZoneInfo(TZ)).isoformat(),
        prompt_type="enhanced",
        efficiency_ratio=quality_enhanced / enhanced_analysis["token_count"],
    )

    prompt_analyzer.save_metrics(atomic_metrics)
    prompt_analyzer.save_metrics(enhanced_metrics)

    return {
        "atomic_prompt": {
            "prompt": atomic_analysis_prompt,
            "analysis": prompt_analysis,
            "quality": quality_atomic,
        },
        "enhanced_prompt": {
            "prompt": enhanced_prompt,
            "analysis": enhanced_analysis,
            "quality": quality_enhanced,
        },
        "comparison": {
            "token_savings": enhanced_analysis["token_count"]
            - prompt_analysis["token_count"],
            "quality_improvement": quality_enhanced - quality_atomic,
            "efficiency_atomic": atomic_metrics.efficiency_ratio,
            "efficiency_enhanced": enhanced_metrics.efficiency_ratio,
        },
    }


def _save_to_local_file(row):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô local file ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡πà‡∏á‡πÑ‡∏õ Google Sheets ‡πÑ‡∏î‡πâ"""
    local_output_file = "chat_analysis_output.json"

    # ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏î‡∏¥‡∏°
    try:
        if os.path.exists(local_output_file):
            with open(local_output_file, "r", encoding="utf-8") as f:
                existing_data = json.load(f)
        else:
            existing_data = []
    except:
        existing_data = []

    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
    existing_data.append(row)

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏•‡∏±‡∏ö
    try:
        with open(local_output_file, "w", encoding="utf-8") as f:
            json.dump(existing_data, f, ensure_ascii=False, indent=2)
        log(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô {local_output_file}")
    except Exception as e:
        log(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ: {e}")


def _save_to_local_file(data):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå local ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á Google Sheets ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ"""
    try:
        import os
        from datetime import datetime

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå backup ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
        backup_dir = "backup_data"
        if not os.path.exists(backup_dir):
            os.makedirs(backup_dir)

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{backup_dir}/chat_export_{timestamp}.json"

        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        log(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå local ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {filename}")
        return filename

    except Exception as e:
        log(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå local ‡πÑ‡∏î‡πâ: {str(e)}")
        return None


def post(row, cache):
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á hash key ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    row_hash = create_row_hash(row)

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏Ñ‡∏¢‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if row_hash in cache:
        log(f"‡∏Ç‡πâ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥: session {row['session_id']}")
        return False

    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏î‡πâ‡∏ß‡∏¢ atomic prompting
    conversation_analysis = analyze_conversation_with_atomic_prompts(
        row["user_prompt"], row["ai_response"]
    )

    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏•‡∏á‡πÉ‡∏ô row
    row["atomic_analysis"] = json.dumps(conversation_analysis, ensure_ascii=False)
    row["token_efficiency"] = conversation_analysis["comparison"]["efficiency_atomic"]
    row["quality_score"] = conversation_analysis["atomic_prompt"]["quality"]

    # Debug: ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡πà‡∏á
    log(f"üì§ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: session {row['session_id']}")
    log(f"   URL: {GS_WEBAPP_URL}")
    log(f"   User prompt: {row['user_prompt'][:50]}...")
    log(
        f"   Quality: {row['quality_score']:.2f}, Efficiency: {row['token_efficiency']:.3f}"
    )

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ URL ‡πÅ‡∏•‡∏∞ API key ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    if (
        GS_WEBAPP_URL == "https://script.google.com/macros/s/XXXX/exec"
        or GS_API_KEY == "my-secret-12345"
    ):
        log("‚ö†Ô∏è  ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ configuration ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô - ‡πÇ‡∏õ‡∏£‡∏î‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç .env file")
        log("   ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô local file ‡πÅ‡∏ó‡∏ô...")
        _save_to_local_file(row)
        return True

    try:
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° api_key ‡∏•‡∏á‡πÉ‡∏ô row ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô v0
        row["api_key"] = GS_API_KEY

        # ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á Google Sheets (‡πÉ‡∏ä‡πâ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö v0)
        response = requests.post(
            GS_WEBAPP_URL,
            json=row,  # ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡πà‡∏≠‡∏î‡πâ‡∏ß‡∏¢ action/apiKey
            headers={"Content-Type": "application/json"},
            timeout=30,
        )

        if response.status_code == 200:
            log(
                f"‚úÖ ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: session {row['session_id']} (Tokens: {conversation_analysis['atomic_prompt']['analysis']['token_count']}, Quality: {row['quality_score']:.2f})"
            )

            # ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö v0 - ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ä‡πá‡∏Ñ JSON response
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÅ‡∏Ñ‡∏ä‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
            cache[row_hash] = {
                "timestamp": row["timestamp"],
                "session_id": row["session_id"],
                "atomic_analysis": conversation_analysis,
            }
            return True
        else:
            log(f"‚ùå HTTP Error {response.status_code}: {response.text[:200]}")
            log("üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô local file ‡πÅ‡∏ó‡∏ô...")
            _save_to_local_file(row)
            return True  # ‡∏¢‡∏±‡∏á‡∏Ñ‡∏∑‡∏ô True ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß

    except requests.exceptions.Timeout:
        log("‚è∞ ‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏´‡∏°‡∏î‡πÄ‡∏ß‡∏•‡∏≤ (timeout)")
        log("üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô local file ‡πÅ‡∏ó‡∏ô...")
        _save_to_local_file(row)
        return True
    except requests.exceptions.RequestException as e:
        log(f"üåê ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {str(e)}")
        log("üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô local file ‡πÅ‡∏ó‡∏ô...")
        _save_to_local_file(row)
        return True


def export_conversations_to_sheet(json_file_path):
    """‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÑ‡∏õ‡∏¢‡∏±‡∏á Google Sheets ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå atomic prompting"""

    log(f"‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå: {json_file_path}")

    # ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏Ñ‡∏ä
    cache = load_cache()

    try:
        with open(json_file_path, "r", encoding="utf-8") as file:
            data = json.load(file)

        if not isinstance(data, list):
            log("‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô array")
            return

        success_count = 0
        total_count = len(data)

        log(f"‡∏û‡∏ö {total_count} ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤")

        for conversation in data:
            try:
                mapping = conversation.get("mapping", {})
                session_id = conversation.get("id", "unknown")
                title = conversation.get("title", "")
                create_t = conversation.get("create_time")

                log(f"üîç ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• conversation: {session_id}")
                log(f"   Title: {title}")
                log(f"   ‡∏û‡∏ö nodes: {len(mapping)} nodes")

                # ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö v0: ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏•‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö
                nodes = list(mapping.values())
                nodes.sort(key=lambda n: n.get("create_time") or 0)

                # ‡∏ß‡∏ô‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏à‡∏≤‡∏Å user/assistant
                user_buf = None
                user_timestamp = None

                for n in nodes:
                    m = n.get("message")
                    if not m:
                        continue

                    role = m.get("author", {}).get("role")
                    parts = m.get("content", {}).get("parts") or []
                    text = "\n".join([p for p in parts if isinstance(p, str)]).strip()

                    if not text:
                        continue

                    log(f"   üìù {role}: {text[:50]}...")

                    if role == "user":
                        user_buf = text
                        user_timestamp = m.get("create_time")
                        log(f"   üë§ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å user message: {text[:100]}...")

                    elif role == "assistant":
                        # ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà user ‚Üí assistant
                        ai_response = text
                        ai_timestamp = m.get("create_time")

                        log(f"   ü§ñ ‡∏û‡∏ö AI response: {text[:100]}...")

                        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡πÑ‡∏õ Google Sheets
                        row = {
                            "session_id": session_id,
                            "user_prompt": user_buf or "",
                            "ai_response": ai_response,
                            "user_timestamp": iso(user_timestamp or create_t),
                            "ai_timestamp": iso(ai_timestamp or create_t),
                            "timestamp": iso(user_timestamp or create_t),
                        }

                        log(f"   üì§ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å post() function...")
                        # ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                        if post(row, cache):
                            success_count += 1
                            log(f"   ‚úÖ post() ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! Total: {success_count}")
                        else:
                            log(f"   ‚ùå post() ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß!")

                        # ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï user buffer ‡∏´‡∏•‡∏±‡∏á‡∏™‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß
                        user_buf = None
                        user_timestamp = None

                        # ‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ API rate limit
                        time.sleep(0.1)

                # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ user ‡∏Ñ‡πâ‡∏≤‡∏á (‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö) ‡∏Å‡πá‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ user
                if user_buf:
                    log(f"   üë§ ‡∏û‡∏ö user message ‡∏Ñ‡πâ‡∏≤‡∏á: {user_buf[:100]}...")
                    row = {
                        "session_id": session_id,
                        "user_prompt": user_buf,
                        "ai_response": "",
                        "user_timestamp": iso(user_timestamp or create_t),
                        "ai_timestamp": iso(create_t),
                        "timestamp": iso(user_timestamp or create_t),
                    }

                    log(f"   üì§ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å post() ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö user ‡∏Ñ‡πâ‡∏≤‡∏á...")
                    if post(row, cache):
                        success_count += 1
                        log(f"   ‚úÖ post() ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! Total: {success_count}")
                    else:
                        log(f"   ‚ùå post() ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß!")

            except Exception as e:
                log(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ {session_id}: {str(e)}")
                continue

        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏Ñ‡∏ä
        save_cache(cache)

        log(f"‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: ‡∏™‡πà‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à {success_count}/{total_count} ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤")

        # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô efficiency
        efficiency_report = prompt_analyzer.get_efficiency_report()
        log("=== Atomic Prompting Efficiency Report ===")
        log(f"Total prompts analyzed: {efficiency_report.get('total_prompts', 0)}")
        log(f"Average tokens per prompt: {efficiency_report.get('avg_tokens', 0):.1f}")
        log(f"Average quality score: {efficiency_report.get('avg_quality', 0):.2f}")
        log(
            f"Average efficiency ratio: {efficiency_report.get('avg_efficiency', 0):.3f}"
        )

        if "comparison" in efficiency_report:
            comp = efficiency_report["comparison"]
            log(
                f"Token savings (enhanced vs atomic): {comp.get('token_savings', 0):.1f}"
            )
            log(f"Quality improvement: {comp.get('quality_difference', 0):.2f}")
            log(f"Efficiency ratio: {comp.get('efficiency_improvement', 0):.3f}")

    except FileNotFoundError:
        log(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {json_file_path}")
    except json.JSONDecodeError:
        log(f"‡πÑ‡∏ü‡∏•‡πå JSON ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: {json_file_path}")
    except Exception as e:
        log(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î: {str(e)}")


if __name__ == "__main__":
    import sys

    if len(sys.argv) != 2:
        print("Usage: python export_to_sheet_v1_atomic.py <json_file_path>")
        print(
            "Example: python export_to_sheet_v1_atomic.py chatgpt-export/conversations-28-09-2025.json"
        )
        sys.exit(1)

    json_file_path = sys.argv[1]
    export_conversations_to_sheet(json_file_path)
