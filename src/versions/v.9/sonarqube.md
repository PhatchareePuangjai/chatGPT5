# TOKEN: squ_e8856635d71cd88b5026fff424a43cb958c255ac

![alt text](image.png)

Abstractâ€”Unstructured prompt-based code generation often fails to produce reliable software in complex domains such as e commerce systems. This paper presents an empirical evaluation of AI-generated code quality by comparing three interaction strategies: Basic Prompting, Context Engineering, and Spec-Driven Development. Using a controlled e-commerce testbed implemented across multiple versions, code quality is evaluated along two dimensions. Functional correctness is assessed through automated acceptance tests and concurrency-focused edge case scenarios, while structural quality and security properties are analyzed using static analysis tools, including SonarQube and CodeQL. Results indicate that although Basic Prompting can satisfy functional requirements in standard execution paths, it frequently degrades under concurrent conditions, exhibiting race conditions and non-atomic operations. Context Engineering improves transaction atomicity and structured error handling and substantially reduces duplication. Spec-Driven Development achieves the strongest overall outcomes, with a cleaner separation of concerns and more reusable helpers, and the best versions reach very low duplication and no detected vulnerabilities. However, across all methods, test coverage remains zero, highlighting the need for systematic automated testing alongside specification-driven workflows. These findings emphasize that structured specification-driven contexts improve reproducibility and robustness for AI-assisted software development.