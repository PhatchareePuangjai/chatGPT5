# Evaluating AI-Generated Code Quality from Basic Prompting to Spec-Driven Development

## Abstract

Unstructured prompt-based code generation often fails to produce reliable software when applied to complex domains such as e-commerce systems. This paper presents an empirical evaluation of AI-generated code quality by comparing three interaction strategies: **Basic Prompting**, **Context Engineering**, and **Specification-Driven Development (SDD)**. Using a controlled e-commerce testbed, code quality is evaluated using **Multi-Dimensional Reporting** across three independent measurement standards: (1) **Functional Correctness Evaluation** through automated acceptance tests and concurrency-focused edge-case scenarios, (2) **SonarQube Static Analysis** for security vulnerabilities, reliability issues, maintainability, and code duplications, and (3) **CodeQL Security Analysis** for security vulnerabilities. Each measurement standard provides objective, reproducible metrics without subjective scoring.

## Key Findings

### 1. Specification-Driven Development (SDD) Shows Variable Results

**Evidence from Multi-Dimensional Reporting:**

**Functional Correctness:**
- v.3-v.11: All tests passed (100%) across all versions
- Basic Prompting (v.3, v.4, v.5): 18 total tests, all passed
- Context Engineering (v.6, v.7, v.8): 18 total tests, all passed
- SDD (v.9, v.10, v.11): 18 total tests, all passed

**SonarQube Analysis:**
- **Best Implementation (v.11):** Security: 0, Reliability: 8, Duplications: 0.0%
- **Worst Implementation (v.10):** Security: 5 (highest), Duplications: 34.8%
- **Average Security:** 2.0 vulnerabilities per SDD implementation (0-5 range)
- **Average Reliability:** 13 issues per SDD implementation
- **Average Duplications:** 12.6% (v.11: 0.0%, v.9: 3.1%, v.10: 34.8%)

**CodeQL Analysis:**
- v.9-v.11: CodeQL reports not available

**Key Observation:** SDD shows the highest variability - best implementation (v.11) achieves excellent metrics, but worst (v.10) has the highest security vulnerabilities across all evaluated versions.

**Characteristics:**
- Specifications act as a single source of truth
- Clean Architecture with clear separation of concerns
- Structured error handling with Error Factory pattern
- Repository Pattern for data access abstraction
- Transaction helpers for atomic operations
- Comprehensive validation through extracted functions

**Key Improvements:**
- Atomic transaction operations (low stock alerts within transactions)
- Structured error codes (`sku_not_found`, `insufficient_stock`, `coupon_expired`)
- Event logging for audit trails
- Money utilities for safe financial calculations

### 2. Context Engineering Achieves Consistent Security and Low Code Duplications

**Evidence from Multi-Dimensional Reporting:**

**SonarQube Analysis:**
- **Security:** 0 vulnerabilities across all implementations (v.6, v.7, v.8) ‚úÖ
- **Code Duplications:** Average 4.5% (v.8: 0.0% ‚úÖ, v.7: 4.3%, v.6: 9.3%)
- **Reliability:** Variable (v.7: 9, v.6: 19, v.8: 17)
- **Maintainability:** Variable (v.7: 12, v.6: 26, v.8: 20)

**CodeQL Analysis:**
- v.6, v.7, v.8: No critical security issues reported ‚úÖ

**Key Observation:** Context Engineering consistently achieves zero security vulnerabilities and significantly reduces code duplications compared to Basic Prompting.

**Characteristics:**
- Better structured error responses with helpful details
- Improved transaction handling and atomicity
- Better SQL organization and comments
- Configurable thresholds from environment variables

**Key Improvements:**
- Structured error codes replacing basic error messages
- Atomic transaction operations
- Better error messages with contextual details
- Improved boundary checks and validation

**Limitations:**
- Still lacks Service Layer separation in some implementations
- No Repository Pattern
- Manual transaction handling (no helpers)

### 3. Basic Prompting Achieves Functional Correctness but Shows High Code Duplications

**Evidence from Multi-Dimensional Reporting:**

**Functional Correctness:**
- v.3, v.4: All tests passed (100%), including critical edge cases (race conditions, transaction atomicity) ‚úÖ

**SonarQube Analysis:**
- **Security:** 0 vulnerabilities across all implementations (v.3, v.4, v.5) ‚úÖ
- **Reliability:** Best (v.5: 1 issue ‚úÖ) to worst (v.4: 9 issues)
- **Code Duplications:** Highly variable (v.5: 1.4% ‚úÖ, v.3: 24.2%, v.4: 65.5% ‚ùå)
- **Average Duplications:** 30.4% (worst: v.4 with 65.5%)

**CodeQL Analysis:**
- v.3, v.4, v.5: No critical security issues reported ‚úÖ

**Key Observation:** Basic Prompting can satisfy functional requirements (100% test pass rate) but shows extreme variability in code duplications (1.4% to 65.5%).

**Characteristics:**
- Can satisfy functional requirements in standard execution paths
- Successfully implements core features (row locking, transactions, validation)
- Monolithic to Service Layer architecture evolution
- Basic error handling with console.error

**Key Issues:**
- **Race Conditions:** Low stock alerts outside transactions (non-atomic operations)
- **Code Duplications:** Extremely high in some implementations (65.5%)
- **Error Handling:** Basic error handling without structured codes
- **Code Organization:** Long all-in-one functions
- **Architecture:** Monolithic structure in early implementations

**Limitations:**
- Fails under concurrent conditions
- Lack of transaction atomicity in critical operations
- Poor code organization leading to high duplications
- Basic error handling without structured codes

## Multi-Dimensional Reporting

This evaluation employs **three independent measurement standards** to assess code quality objectively, without subjective scoring. Each standard provides reproducible, quantitative metrics.

---

### 1. Functional Correctness Evaluation

**Methodology:** Automated acceptance tests using Jest, covering scenarios defined in `scenarios_*.md` files for each version. Tests include both standard scenarios and edge cases (race conditions, transaction atomicity, boundary values).

**Results by Version:**

| Version | Development Method | Test Scenarios | Pass Rate | Key Test Cases |
|---------|-------------------|----------------|-----------|----------------|
| **v.3** | Basic Prompting | 7 tests | ‚úÖ 100% (7/7) | Stock deduction, Low stock alert, Race condition, Transaction atomicity, Overselling prevention, Boundary values |
| **v.4** | Basic Prompting | 5 tests | ‚úÖ 100% (5/5) | Update quantity, Merge items, Save for later, Add more than stock, Floating point calculation |
| **v.5** | Basic Prompting | 6 tests | ‚úÖ 100% (6/6) | Coupon validation, Cart total discount %, Expiration check, Usage limit, Order of operations, Negative total protection |
| **v.6** | Context Engineering | 7 tests | ‚úÖ 100% (7/7) | Stock deduction, Low stock alert, Stock restoration, Race condition, Transaction atomicity, Overselling prevention, Boundary values |
| **v.7** | Context Engineering | 5 tests | ‚úÖ 100% (5/5) | Update quantity, Merge items, Save for later, Add more than stock, Floating point calculation |
| **v.8** | Context Engineering | 6 tests | ‚úÖ 100% (6/6) | Coupon validation, Cart total discount %, Expiration check, Usage limit, Order of operations, Negative total protection |
| **v.9** | SDD | 7 tests | ‚úÖ 100% (7/7) | Stock deduction, Low stock alert, Stock restoration, Race condition, Transaction atomicity, Overselling prevention, Boundary values |
| **v.10** | SDD | 5 tests | ‚úÖ 100% (5/5) | Update quantity, Merge items, Save for later, Add more than stock, Floating point calculation |
| **v.11** | SDD | 6 tests | ‚úÖ 100% (6/6) | Coupon validation, Cart total discount %, Expiration check, Usage limit, Order of operations, Negative total protection |

**Key Findings:**
- ‚úÖ **v.3 (Basic Prompting)**: All 7 tests passed, including critical edge cases (race conditions, transaction atomicity)
- ‚úÖ **v.4 (Basic Prompting)**: All 5 tests passed, including floating point calculation precision
- ‚úÖ **v.5 (Basic Prompting)**: All 6 tests passed, including coupon validation and order of operations
- ‚úÖ **v.6 (Context Engineering)**: All 7 tests passed, including race conditions and transaction atomicity
- ‚úÖ **v.7 (Context Engineering)**: All 5 tests passed, including merge items logic and stock validation
- ‚úÖ **v.8 (Context Engineering)**: All 6 tests passed, including coupon validation and negative total protection
- ‚úÖ **v.9 (SDD)**: All 7 tests passed, including race conditions and transaction atomicity with `withTransaction` helper
- ‚úÖ **v.10 (SDD)**: All 5 tests passed, including merge items logic and floating point calculation
- ‚úÖ **v.11 (SDD)**: All 6 tests passed, including complex scenarios (order of operations, negative total protection)

**Edge Case Performance:**
- **Race Conditions**: v.3, v.6, v.9 successfully handled 5 concurrent requests (only 1 success, 4 failures, stock did not go negative)
- **Transaction Atomicity**: v.3, v.6, v.9 verified rollback on DB error (v.9 uses `withTransaction` helper)
- **Overselling Prevention**: v.3, v.4, v.6, v.7, v.9, v.10 correctly rejected requests exceeding available stock
- **Floating Point Precision**: v.4, v.7, v.10 correctly handled integer cents calculation (19.99 * 3 = 59.97)
- **Coupon Validation**: v.5, v.8, v.11 correctly validated coupons (expiration, usage limits, minimum purchase)
- **Order of Operations**: v.5, v.8, v.11 correctly applied discounts in sequence (percent then fixed)
- **Negative Total Protection**: v.5, v.8, v.11 correctly capped totals at 0 when discount exceeds subtotal

---

### 2. SonarQube Static Analysis

**Methodology:** Automated static analysis using SonarQube across all versions. Metrics include Security Vulnerabilities, Reliability Issues (Bugs), Maintainability Issues (Code Smells), Security Hotspots, Test Coverage, and Code Duplications.

**Aggregate Results (All Versions):**
- **Lines of Code:** 9,285
- **Security Vulnerabilities:** 6 ‚ö†Ô∏è
- **Reliability Issues:** 101 ‚ö†Ô∏è
- **Maintainability Issues:** 148 ‚ö†Ô∏è
- **Security Hotspots:** 44 üîç
- **Test Coverage:** 0.0% ‚ùå
- **Code Duplications:** 16.6% ‚ö†Ô∏è

**Results by Version:**

| Version | Dev Method | LoC | Security | Reliability | Maintainability | Hotspots | Coverage | Duplications |
|---------|-----------|-----|----------|-------------|----------------|----------|----------|--------------|
| **v.2** | - | 1,239 | 0 ‚úÖ | 4 ‚úÖ | 8 ‚úÖ | 4 ‚ö†Ô∏è | 0.0% ‚ùå | 23.4% ‚ö†Ô∏è |
| **v.3** | Basic Prompting | 717 | 0 ‚úÖ | 3 ‚úÖ | 7 ‚úÖ | 2 ‚úÖ | 0.0% ‚ùå | 24.2% ‚ö†Ô∏è |
| **v.4** | Basic Prompting | 753 | 0 ‚úÖ | 9 ‚ö†Ô∏è | 18 ‚ö†Ô∏è | 4 ‚ö†Ô∏è | 0.0% ‚ùå | **65.5%** ‚ùå |
| **v.5** | Basic Prompting | 1,161 | 0 ‚úÖ | **1** ‚úÖ | 10 ‚úÖ | 5 ‚ö†Ô∏è | 0.0% ‚ùå | 1.4% ‚úÖ |
| **v.6** | Context Engineering | 1,351 | 0 ‚úÖ | 19 ‚ö†Ô∏è | **26** ‚ö†Ô∏è | 7 ‚ö†Ô∏è | 0.0% ‚ùå | 9.3% ‚úÖ |
| **v.7** | Context Engineering | 834 | 0 ‚úÖ | 9 ‚ö†Ô∏è | 12 ‚ö†Ô∏è | 3 ‚úÖ | 0.0% ‚ùå | 4.3% ‚úÖ |
| **v.8** | Context Engineering | 644 | 0 ‚úÖ | 17 ‚ö†Ô∏è | 20 ‚ö†Ô∏è | 5 ‚ö†Ô∏è | 0.0% ‚ùå | **0.0%** ‚úÖ |
| **v.9** | SDD | 952 | 1 ‚ö†Ô∏è | **22** ‚ùå | 18 ‚ö†Ô∏è | 4 ‚ö†Ô∏è | 0.0% ‚ùå | 3.1% ‚úÖ |
| **v.10** | SDD | 826 | **5** ‚ùå | 9 ‚ö†Ô∏è | 12 ‚ö†Ô∏è | 4 ‚ö†Ô∏è | 0.0% ‚ùå | 34.8% ‚ö†Ô∏è |
| **v.11** | SDD | 808 | 0 ‚úÖ | 8 ‚úÖ | 17 ‚ö†Ô∏è | 6 ‚ö†Ô∏è | 0.0% ‚ùå | **0.0%** ‚úÖ |

**Key Findings by Development Method:**

**Basic Prompting (v.3, v.4, v.5):**
- **Security:** 0 vulnerabilities across all implementations ‚úÖ
- **Reliability:** Best (v.5: 1 issue) to worst (v.4: 9 issues)
- **Code Duplications:** Highly variable (v.5: 1.4% ‚úÖ, v.4: 65.5% ‚ùå)
- **Best:** v.5 (Reliability: 1, Duplications: 1.4%)
- **Worst:** v.4 (Duplications: 65.5%)

**Context Engineering (v.6, v.7, v.8):**
- **Security:** 0 vulnerabilities across all implementations ‚úÖ
- **Reliability:** Variable (v.7: 9, v.6: 19, v.8: 17)
- **Code Duplications:** Improved (v.8: 0.0% ‚úÖ, v.7: 4.3%, v.6: 9.3%)
- **Best:** v.8 (Duplications: 0.0%, Security: 0)
- **Worst:** v.6 (Reliability: 19, Maintainability: 26)

**Specification-Driven Development (v.9, v.10, v.11):**
- **Security:** Variable (v.11: 0 ‚úÖ, v.9: 1 ‚ö†Ô∏è, v.10: 5 ‚ùå)
- **Reliability:** Variable (v.11: 8 ‚úÖ, v.10: 9, v.9: 22 ‚ùå)
- **Code Duplications:** Best (v.11: 0.0% ‚úÖ, v.9: 3.1%, v.10: 34.8% ‚ö†Ô∏è)
- **Best:** v.11 (Security: 0, Duplications: 0.0%, Reliability: 8)
- **Worst:** v.10 (Security: 5, Duplications: 34.8%), v.9 (Reliability: 22)

**Critical Issues Identified:**
- ‚ùå **Test Coverage:** 0.0% across ALL versions
- ‚ùå **v.10:** 5 security vulnerabilities (highest)
- ‚ùå **v.9:** 22 reliability issues (highest)
- ‚ùå **v.4:** 65.5% code duplications (highest)
- ‚ùå **v.6:** 26 maintainability issues (highest)

---

### 3. CodeQL Security Analysis

**Methodology:** Automated security analysis using CodeQL for static security vulnerability detection.

**Results by Version:**

| Version | Development Method | CodeQL Analysis | Status |
|---------|-------------------|-----------------|--------|
| **v.3** | Basic Prompting | Analysis performed | ‚úÖ No critical issues reported |
| **v.4** | Basic Prompting | Analysis performed | ‚úÖ No critical issues reported |
| **v.5** | Basic Prompting | Analysis performed | ‚úÖ No critical issues reported |
| **v.6** | Context Engineering | Analysis performed | ‚úÖ No critical issues reported |
| **v.7** | Context Engineering | Analysis performed | ‚úÖ No critical issues reported |
| **v.8** | Context Engineering | Analysis performed | ‚úÖ No critical issues reported |
| **v.9** | SDD | ‚ö†Ô∏è No CodeQL report | - |
| **v.10** | SDD | ‚ö†Ô∏è No CodeQL report | - |
| **v.11** | SDD | ‚ö†Ô∏è No CodeQL report | - |

**Key Findings:**
- ‚úÖ **v.3-v.8:** CodeQL analysis performed, no critical security issues reported
- ‚ö†Ô∏è **v.9-v.11:** CodeQL reports not available
- **Note:** CodeQL results align with SonarQube security findings for v.3-v.8 (0 vulnerabilities)

---

## Experimental Results

### Summary Comparison Across Measurement Standards

| Development Method | Avg Quality Score | Avg Code Duplications | Security Vulnerabilities | Best Reliability | Architecture Evolution |
|-------------------|------------------|----------------------|------------------------|------------------|----------------------|
| **Basic Prompting** | 3.33‚≠ê | 30.4% | 0 (all implementations) ‚úÖ | 1 issue (best) | Monolithic ‚Üí Service Layer |
| **Context Engineering** | 3.67‚≠ê | 4.5% | 0 (all implementations) ‚úÖ | 9 issues | Better Structure |
| **Specification-Driven Development** | 4.33‚≠ê | 12.6% | 0-5 (avg: 2.0) ‚ö†Ô∏è | 8 issues | Clean Architecture ‚úÖ |

**Note on Security:** While Basic Prompting and Context Engineering achieved 0 vulnerabilities across all implementations, SDD shows variability: best implementation (v.11) has 0, but worst implementation (v.10) has 5 vulnerabilities (highest across all evaluated versions).

### Rubric-Based Quality Score Methodology

**Quality Score** is a 1-5 star rating (‚≠ê) derived from a **predefined rubric** with anchored criteria per score level. Ratings were performed using a structured scoring system to ensure consistency and reproducibility.

#### Scoring Rubric (4 Dimensions √ó 0-3 Points)

Each dimension is scored independently on a 0-3 scale, then aggregated to produce the final star rating.

**Table 1: Quality Score Rubric**

| Dimension | 0 Points | 1 Point | 2 Points | 3 Points |
|-----------|----------|---------|----------|----------|
| **1. Architecture** | Monolithic, no separation | Basic structure, some organization | Service Layer or clear separation | Clean Architecture (Repository + Service Pattern) |
| **2. Features** | No validation, basic error handling | Basic validation or error handling | Good validation + error handling | Comprehensive: Zod + Error Factory + Money utils + Tests |
| **3. Code Organization** | Poor structure, high duplication (>30%) | Basic structure, moderate duplication (10-30%) | Good structure, low duplication (<10%) | Excellent structure, zero duplication (0%) |
| **4. Best Practices** | No best practices | Some best practices | Good best practices | Comprehensive best practices (patterns, utilities, structured errors) |

**Scoring Rules:**
- Each dimension: 0-3 points (total possible: 12 points)
- **Penalty Rules:**
  - If Security vulnerabilities > 0: Maximum score capped at 4‚≠ê (regardless of rubric score)
  - If Code Duplications > 50%: Architecture dimension capped at 1 point
  - If Code Duplications > 30%: Code Organization dimension capped at 1 point

**Score to Star Rating Mapping:**

| Total Score | Star Rating | Description |
|-------------|-------------|-------------|
| 0-3 points | ‚≠ê (1/5) | Poor - Not production-ready |
| 4-6 points | ‚≠ê‚≠ê (2/5) | Basic - Missing critical features |
| 7-9 points | ‚≠ê‚≠ê‚≠ê (3/5) | Acceptable - Basic structure, some best practices |
| 10-11 points | ‚≠ê‚≠ê‚≠ê‚≠ê (4/5) | Good - Service Layer, good practices |
| 12 points | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5) | Excellent - Clean Architecture, comprehensive practices |

**Quality Score by Version (with Rubric Scores):**

| Version | Architecture | Features | Organization | Best Practices | Total | Star Rating | Original Rating | Dev Method |
|---------|-------------|----------|--------------|----------------|-------|-------------|-----------------|------------|
| v.3 | 1 | 1 | 1 | 1 | 4 | ‚≠ê‚≠ê (2/5) | ‚≠ê‚≠ê‚≠ê (3/5) | Basic Prompting |
| v.4 | 2 | 2 | 0* | 2 | 6 | ‚≠ê‚≠ê (2/5)* | ‚≠ê‚≠ê‚≠ê‚≠ê (4/5) | Basic Prompting |
| v.5 | 1 | 1 | 2 | 1 | 5 | ‚≠ê‚≠ê (2/5) | ‚≠ê‚≠ê‚≠ê (3/5) | Basic Prompting |
| v.6 | 2 | 2 | 1 | 2 | 7 | ‚≠ê‚≠ê‚≠ê (3/5) | ‚≠ê‚≠ê‚≠ê‚≠ê (4/5) | Context Engineering |
| v.7 | 1 | 1 | 1 | 1 | 4 | ‚≠ê‚≠ê (2/5) | ‚≠ê‚≠ê‚≠ê (3/5) | Context Engineering |
| v.8 | 2 | 2 | 3 | 2 | 9 | ‚≠ê‚≠ê‚≠ê (3/5) | ‚≠ê‚≠ê‚≠ê‚≠ê (4/5) | Context Engineering |
| v.9 | 2 | 2 | 2 | 2 | 8 | ‚≠ê‚≠ê‚≠ê (3/5) | ‚≠ê‚≠ê‚≠ê‚≠ê (4/5) | SDD |
| v.10 | 2 | 2 | 1* | 2 | 7 | ‚≠ê‚≠ê‚≠ê (3/5)* | ‚≠ê‚≠ê‚≠ê‚≠ê (4/5) | SDD |
| v.11 | 3 | 3 | 3 | 3 | 12 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5) | SDD |

*Note: v.4 and v.10 have high code duplications (>30%), so Organization dimension is penalized per rubric rules.

**Explanation of Rating Differences:**
- **Original Ratings**: Holistic expert assessment based on overall impression
- **Rubric-Based Ratings**: Systematic scoring using predefined criteria
- **Discrepancy Analysis**: 
  - v.4: Original 4‚≠ê (high features) vs Rubric 2‚≠ê (penalized for 65.5% duplications)
  - v.6, v.8, v.9, v.10: Original 4‚≠ê (good overall) vs Rubric 3‚≠ê (systematic assessment)
  - v.11: Consistent 5‚≠ê (excellent in all dimensions)

**Average Quality Score:**
- **Original Holistic Ratings:**
  - Basic Prompting: (3 + 4 + 3) / 3 = **3.33‚≠ê**
  - Context Engineering: (4 + 3 + 4) / 3 = **3.67‚≠ê**
  - Specification-Driven Development: (4 + 4 + 5) / 3 = **4.33‚≠ê**
- **Rubric-Based Ratings:**
  - Basic Prompting: (2 + 2 + 2) / 3 = **2.0‚≠ê**
  - Context Engineering: (3 + 2 + 3) / 3 = **2.67‚≠ê**
  - Specification-Driven Development: (3 + 3 + 5) / 3 = **3.67‚≠ê**

**For Research Reporting:**
- **Primary Analysis**: Use rubric-based scores for objectivity and reproducibility
- **Secondary Analysis**: Report original holistic ratings for context
- **Main Text**: Report rubric-based averages with explanation of methodology

**Note on Methodology:**
- This rubric-based scoring system provides a more objective and reproducible assessment compared to holistic star ratings
- The rubric was applied retrospectively to all versions for consistency
- Primary outcomes remain quantitative metrics (SonarQube, CodeQL, acceptance tests)
- Quality Score serves as a secondary outcome focusing on architecture and best practices

---

#### Scoring Process and Limitations

**Scoring Procedure:**
1. **Single Rater Assessment**: All versions were evaluated by a single expert reviewer using the predefined rubric (Table 1)
2. **Rubric Application**: Each version was scored systematically using the rubric criteria (4 dimensions √ó 0-3 points)
3. **Retrospective Application**: The rubric was developed and applied retrospectively to ensure consistent application across all versions
4. **Documentation**: All scoring decisions were documented with rationale based on observable code characteristics

**Important Limitations:**
- ‚ö†Ô∏è **Single Rater**: This study used a single expert reviewer. No inter-rater reliability measures (Cohen's Kappa, ICC) were calculated.
- ‚ö†Ô∏è **No Test-Retest**: Test-retest reliability was not assessed.
- ‚ö†Ô∏è **Potential Bias**: Single-rater assessment may introduce personal bias, though the rubric-based approach aims to minimize subjectivity.
- ‚ö†Ô∏è **Retrospective Application**: The rubric was developed after initial holistic assessments, which may affect scoring consistency.

**Mitigation Strategies:**
- **Rubric-Based Scoring**: Using a predefined rubric with clear criteria reduces subjectivity compared to holistic ratings
- **Transparent Criteria**: All scoring criteria are explicitly defined and documented (see Table 1)
- **Observable Evidence**: Scores are based on observable code characteristics (presence of patterns, libraries, structure) rather than subjective impressions
- **Reproducibility**: The rubric enables other researchers to replicate the scoring process

**Recommendations for Future Work:**
- **Multiple Raters**: Future studies should employ at least 2 independent reviewers to assess inter-rater reliability
- **Blind Scoring**: Reviewers should be blinded to the development method to reduce bias
- **Test-Retest**: Re-scoring a subset of versions after a time interval would assess consistency
- **Consensus Building**: Discrepancies between raters should be discussed to reach consensus

**Current Study Limitations:**
- Quality Score is a **secondary outcome** focusing on architecture-centric assessment
- It may not always align with quantitative metrics (e.g., code duplications, security vulnerabilities)
- The rubric emphasizes structural quality and best practices over bug counts or security issues
- For comprehensive evaluation, Quality Score should be interpreted alongside SonarQube metrics and acceptance test results
- **Single-rater assessment limits generalizability** - results should be interpreted with caution regarding inter-rater reliability

---

#### Scope and Interpretation of Quality Score

**What Quality Score Measures:**
- ‚úÖ Architecture patterns and code structure
- ‚úÖ Implementation of best practices (validation, error handling, design patterns)
- ‚úÖ Code organization and modularity
- ‚úÖ Adherence to industry standards

**What Quality Score Does NOT Measure:**
- ‚ùå Security vulnerabilities (measured separately by SonarQube/CodeQL)
- ‚ùå Functional correctness (measured by acceptance tests)
- ‚ùå Performance characteristics
- ‚ùå Test coverage (measured separately)

**Why Quality Score May Differ from Quantitative Metrics:**

1. **Architecture-Centric Focus**: Quality Score prioritizes structural quality and design patterns. A version with good architecture (4‚≠ê) may still have high code duplications if the duplication occurs within well-structured modules.

2. **Best Practices vs. Bug Counts**: Quality Score rewards the presence of best practices (e.g., Error Factory, Repository Pattern) even if some implementations have bugs. Bugs are measured separately by SonarQube Reliability metrics.

3. **Example: v.4 (4‚≠ê Quality Score, 65.5% Duplications)**
   - Quality Score: 4‚≠ê because it has Service Layer, Zod validation, ES Modules, Row Locking
   - SonarQube: High duplications (65.5%) due to copy-paste patterns
   - **Interpretation**: Good architecture and features, but poor code reuse

4. **Example: v.11 (5‚≠ê Quality Score, 0 vulnerabilities)**
   - Quality Score: 5‚≠ê because it has Clean Architecture, Error Factory, Repository Pattern
   - SonarQube: 0 vulnerabilities, 0% duplications
   - **Interpretation**: Excellent alignment between structural quality and quantitative metrics

**Recommendation for Research Use:**
- Report Quality Score as a **secondary outcome** alongside primary quantitative metrics
- Include the rubric (Table 1) in the appendix for reproducibility
- **Explicitly acknowledge single-rater limitation** (no inter-rater reliability measures)
- Acknowledge limitations and scope explicitly
- Recommend future work with multiple raters to assess reliability

---

### ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Quality Score (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)

**Quality Score** ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÇ‡∏Ñ‡πâ‡∏î‡πÅ‡∏ö‡∏ö‡πÄ‡∏ä‡∏¥‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û (Qualitative Assessment) ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô 1-5 ‡∏î‡∏≤‡∏ß (‚≠ê) ‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏ö‡∏ó‡∏ß‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î (Code Review) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î

#### ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (Evaluation Criteria)

‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Quality Score ‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡∏ì‡∏ë‡πå 4 ‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏•‡∏±‡∏Å:

1. **‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏° (Architecture)**
   - ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î (Code structure)
   - ‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏ö‡∏ú‡∏¥‡∏î‡∏ä‡∏≠‡∏ö (Separation of concerns)
   - Design patterns ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ (Repository Pattern, Service Layer, Clean Architecture)
   - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: v.11 ‡πÑ‡∏î‡πâ 5‚≠ê ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÉ‡∏ä‡πâ Clean Architecture (Repository + Service Pattern)

2. **‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡πÅ‡∏•‡∏∞ Features (Features & Best Practices)**
   - ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ validation library (‡πÄ‡∏ä‡πà‡∏ô Zod)
   - Error handling ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö (Error Factory, Error Middleware)
   - Transaction management (Transaction helpers, Atomic operations)
   - Concurrency safety (Row locking, FOR UPDATE)
   - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: v.4 ‡πÑ‡∏î‡πâ 4‚≠ê ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏µ Zod validation ‡πÅ‡∏•‡∏∞ Transaction helper

3. **‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡πÇ‡∏Ñ‡πâ‡∏î (Code Organization)**
   - ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå
   - Modularity ‡πÅ‡∏•‡∏∞ reusability
   - Maintainability
   - Code duplications
   - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: v.8 ‡πÑ‡∏î‡πâ 4‚≠ê ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ Code Duplications = 0.0%

4. **‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞ Best Practices (Standards & Best Practices)**
   - ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ ES Modules vs CommonJS
   - Type safety (TypeScript, Zod)
   - Money handling (Integer cents/satang math)
   - Structured logging
   - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: v.11 ‡πÑ‡∏î‡πâ 5‚≠ê ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏µ Error Factory, Repository Pattern, Structured logging

#### ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (Scoring Scale)

- **‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (5/5)**: Clean Architecture, Best practices ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô, Code organization ‡∏î‡∏µ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°
  - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: v.11 - Repository Pattern, Error Factory, Clean Architecture
  - **‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå 4 ‡∏î‡πâ‡∏≤‡∏ô (v.11):**
    
    **1. Architecture (5/5) - ‡∏î‡∏µ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°**
    - ‚úÖ **Clean Architecture**: ‡πÅ‡∏¢‡∏Å layers ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (Repository + Service + Controller)
    - ‚úÖ **Repository Pattern**: ‡πÅ‡∏¢‡∏Å data access layer (`cartRepository.js`, `couponRepository.js`, `eventRepository.js`)
    - ‚úÖ **Service Layer**: ‡πÅ‡∏¢‡∏Å business logic (`cartService.js`, `promotionService.js`)
    - ‚úÖ **Separation of Concerns**: ‡πÅ‡∏¢‡∏Å concerns ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (routes, controllers, services, repositories, utils)
    - ‚úÖ **‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå**: ‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö
    
    **2. Features (5/5) - ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô**
    - ‚úÖ **Zod Validation**: Type-safe input validation (Zod 3.23.8)
    - ‚úÖ **Error Factory Pattern**: Structured error handling (`errorFactory.expired()`, `errorFactory.minSpend()`)
    - ‚úÖ **Money Utilities**: Safe financial calculations (`sumCents`, `clampZero`, `percentOf`)
    - ‚úÖ **Transaction Management**: Atomic operations (via Repository pattern)
    - ‚úÖ **Comprehensive Tests**: ‡∏°‡∏µ test files (`scenarios.test.js`, `promotionService.test.js`, `money.test.js`)
    - ‚úÖ **Error Responses**: ‡∏°‡∏µ structured error responses (`error` ‡πÅ‡∏•‡∏∞ `reason` fields)
    
    **3. Code Organization (5/5) - ‡∏î‡∏µ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°**
    - ‚úÖ **‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô**: ‡πÅ‡∏¢‡∏Å folders ‡∏ï‡∏≤‡∏°‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà (routes, controllers, services, repositories, utils, middleware)
    - ‚úÖ **Modularity**: ‡πÅ‡∏¢‡∏Å modules ‡∏ï‡∏≤‡∏° responsibility
    - ‚úÖ **Reusability**: ‡∏°‡∏µ utilities ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ã‡πâ‡∏≥‡πÑ‡∏î‡πâ (money.js, errors.js)
    - ‚úÖ **Code Duplications**: 0.0% (‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î - ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏Ñ‡πâ‡∏î‡∏ã‡πâ‡∏≥)
    - ‚úÖ **Maintainability**: ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏á‡πà‡∏≤‡∏¢‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£ maintain ‡πÅ‡∏•‡∏∞ extend
    
    **4. Best Practices (5/5) - ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô**
    - ‚úÖ **Error Factory Pattern**: Structured error handling ‡πÅ‡∏ó‡∏ô basic error messages
    - ‚úÖ **Repository Pattern**: Clean data access layer
    - ‚úÖ **Money Utilities**: Integer-based calculations (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô floating-point errors)
    - ‚úÖ **Structured Error Handling**: Error responses ‡∏ó‡∏µ‡πà‡∏°‡∏µ structure (`error`, `reason`)
    - ‚úÖ **Type Safety**: Zod validation ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö runtime type checking
    - ‚úÖ **Comprehensive Testing**: ‡∏°‡∏µ test coverage ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö scenarios, services, ‡πÅ‡∏•‡∏∞ utilities
    
  - **‡∏™‡∏£‡∏∏‡∏õ**: v.11 ‡πÑ‡∏î‡πâ 5‚≠ê ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ó‡∏±‡πâ‡∏á 4 ‡∏î‡πâ‡∏≤‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏î‡∏µ‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏° (5/5) ‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡∏î‡πâ‡∏≤‡∏ô
  - **‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏**: ‡πÅ‡∏°‡πâ v.11 ‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ CommonJS (‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô ES Modules) ‡πÅ‡∏ï‡πà‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡πÉ‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏°‡∏≤‡∏Å‡∏à‡∏ô‡πÑ‡∏î‡πâ 5‚≠ê ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Clean Architecture ‡πÅ‡∏•‡∏∞ Code Duplications = 0.0%

- **‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ (4/5)**: ‡∏°‡∏µ Service Layer, Best practices ‡∏î‡∏µ, ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ñ‡∏∂‡∏á Clean Architecture
  - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: v.4, v.6, v.8, v.9, v.10 - ‡∏°‡∏µ features ‡∏î‡∏µ ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á
  - **‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå 4 ‡∏î‡πâ‡∏≤‡∏ô:**
    - ‚ö†Ô∏è **Architecture (4/5)**: ‡∏°‡∏µ Service Layer ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ Repository Pattern (Clean Architecture)
    - ‚úÖ **Features (4/5)**: ‡∏°‡∏µ features ‡∏î‡∏µ ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô (‡πÄ‡∏ä‡πà‡∏ô v.4, v.10 ‡∏°‡∏µ Zod ‡πÅ‡∏ï‡πà v.6, v.7, v.8, v.9 ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ)
    - ‚ö†Ô∏è **Code Organization (4/5)**: ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏î‡∏µ ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏°‡∏µ Code Duplications (v.4: 65.5%, v.10: 34.8%)
    - ‚ö†Ô∏è **Best Practices (4/5)**: ‡∏°‡∏µ Best practices ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö (‡πÄ‡∏ä‡πà‡∏ô v.8 ‡∏°‡∏µ Error Middleware ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ Error Factory)

- **‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ (3/5)**: ‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô, Features ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô, ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏Ç‡∏≤‡∏î Best practices
  - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: v.3, v.5, v.7 - ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ validation ‡∏´‡∏£‡∏∑‡∏≠ error handling ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö
  - **‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ï‡∏≤‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå 4 ‡∏î‡πâ‡∏≤‡∏ô:**
    - ‚ö†Ô∏è **Architecture (3/5)**: Monolithic ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô, ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ Service Layer ‡∏´‡∏£‡∏∑‡∏≠ Clean Architecture
    - ‚ö†Ô∏è **Features (3/5)**: ‡∏°‡∏µ features ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (Docker, Tests) ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ validation, error handling ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö
    - ‚ö†Ô∏è **Code Organization (3/5)**: ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô, ‡∏≠‡∏≤‡∏à‡∏°‡∏µ Code Duplications
    - ‚ö†Ô∏è **Best Practices (3/5)**: ‡∏¢‡∏±‡∏á‡∏Ç‡∏≤‡∏î Best practices ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (validation, structured error handling)

- **‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ (2/5)**: ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô, ‡∏¢‡∏±‡∏á‡∏Ç‡∏≤‡∏î features ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
  - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: v.2 - ‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ Docker, validation, error handling

- **‚≠ê‚òÜ‚òÜ‚òÜ‚òÜ (1/5)**: ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå, ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

#### ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (Average Calculation)

‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ Quality Score ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô:

**Basic Prompting (v.3, v.4, v.5):**
- v.3: 3‚≠ê (‡∏°‡∏µ Docker, Tests ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ Validation)
- v.4: 4‚≠ê (‡∏°‡∏µ Zod, ES Modules, Row Locking)
- v.5: 3‚≠ê (TypeScript Frontend ‡πÅ‡∏ï‡πà Backend ‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ CommonJS)
- **‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: (3 + 4 + 3) / 3 = 3.33‚≠ê**

**Context Engineering (v.6, v.7, v.8):**
- v.6: 4‚≠ê (Concurrency Safety, Atomic Transactions)
- v.7: 3‚≠ê (Stock Guard, Integer Cents Math)
- v.8: 4‚≠ê (Error Middleware, Money Utils, Code Duplications = 0.0%)
- **‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: (4 + 3 + 4) / 3 = 3.67‚≠ê**

**Specification-Driven Development (v.9, v.10, v.11):**
- v.9: 4‚≠ê (API Endpoints, Alerts, Service Layer)
- v.10: 4‚≠ê (Zod, ES Modules, Service Layer)
- v.11: 5‚≠ê (Repository Pattern, Error Factory, Clean Architecture)
- **‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: (4 + 4 + 5) / 3 = 4.33‚≠ê**

#### ‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç

1. **Quality Score ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û** ‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Å‡∏±‡∏ö metrics ‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏à‡∏≤‡∏Å SonarQube (Security, Reliability, Maintainability, Duplications)

2. **‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å Code Review** ‡∏ó‡∏µ‡πà‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Architecture, Patterns, Features, ‡πÅ‡∏•‡∏∞ Best Practices ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î

3. **Quality Score ‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°** ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏ô‡∏î‡πâ‡∏≤‡∏ô Architecture ‡πÅ‡∏•‡∏∞ Best Practices ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ metrics ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏î‡πâ‡∏≤‡∏ô

4. **‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å SonarQube metrics** ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å:
   - Quality Score ‡πÄ‡∏ô‡πâ‡∏ô Architecture ‡πÅ‡∏•‡∏∞ Best Practices
   - SonarQube ‡πÄ‡∏ô‡πâ‡∏ô Security, Reliability, Maintainability issues
   - ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: v.4 ‡πÑ‡∏î‡πâ Quality Score 4‚≠ê ‡πÅ‡∏ï‡πà SonarQube ‡∏û‡∏ö Code Duplications 65.5%

#### ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢

**‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á:** ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Quality Score ‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢ ‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ö‡∏∏‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏ß‡πà‡∏≤:

1. **‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (Scoring Methodology)**: ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ó‡∏±‡πâ‡∏á 4 ‡∏î‡πâ‡∏≤‡∏ô (Architecture, Features, Code Organization, Best Practices) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î

2. **‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô (Evaluators)**: ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤‡πÉ‡∏Ñ‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (Code Reviewer) ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
   - **‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏**: ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (single rater) - ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô inter-rater reliability

3. **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠ (Reliability)**: ‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ‡∏Å‡∏≤‡∏£ cross-check ‡∏Å‡∏±‡∏ö SonarQube metrics ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
   - **‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î**: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô inter-rater reliability (Cohen's Kappa, ICC) ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏°‡∏µ‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
   - **‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î**: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô test-retest reliability

4. **‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î (Limitations)**: 
   - Quality Score ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û ‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ï‡∏ô‡∏±‡∏¢ (subjective)
   - **Single-rater assessment** ‡∏≠‡∏≤‡∏à‡∏°‡∏µ bias - ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö metrics ‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì
   - ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö metrics ‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì (SonarQube, CodeQL)

5. **Reproducibility**: ‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡∏≠‡∏∑‡πà‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏ã‡πâ‡∏≥‡πÑ‡∏î‡πâ (reproducible)
   - Rubric-based scoring ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏° reproducibility ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ holistic ratings
   - **‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥**: ‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 2 ‡∏Ñ‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô inter-rater reliability

### Architecture Evolution

**Basic Prompting:**
- Monolithic structure
- Direct SQL queries in business logic
- Manual transaction handling
- Basic error handling

**Context Engineering:**
- Better structure with organized steps
- Improved SQL organization
- Better transaction handling
- Structured error codes

**Specification-Driven Development:**
- Clean Architecture (Repository + Service Pattern)
- Repository Pattern for data access
- Transaction helpers (`withTransaction`, `withTx`)
- Error Factory pattern
- Extracted validation functions

### Error Handling Evolution

**Basic Prompting:**
- `console.error()` for error logging
- Error arrays (`errors.push()`)
- Basic error messages
- No structured error codes

**Context Engineering:**
- Structured error codes (`BAD_REQUEST`, `NOT_FOUND`, `INSUFFICIENT_STOCK`)
- Better error messages with contextual details
- `apiError()` helper function
- Helpful error details (e.g., `maxAddQty`)

**Specification-Driven Development:**
- Error Factory pattern (`errorFactory.expired()`, `errorFactory.minSpend()`)
- Structured error codes with consistent format
- Event logging for error tracking
- Comprehensive error handling

### Transaction & Atomicity

**Basic Prompting:**
- Manual transaction handling (BEGIN/COMMIT/ROLLBACK)
- Critical operations outside transactions (non-atomic)
- Example: Low stock alerts outside transaction

**Context Engineering:**
- Manual transaction handling
- Critical operations moved inside transactions (atomic)
- Example: Low stock alerts inside transaction

**Specification-Driven Development:**
- Transaction helpers (`withTransaction()`, `withTx()`)
- All critical operations within transactions
- Repository pattern ensures atomicity
- Example: Low stock alerts within transaction + Repository Pattern

### Code Organization

**Basic Prompting:**
- Long all-in-one functions
- Mixed validation in business logic
- Direct SQL queries
- High code duplications (up to 65.5%)

**Context Engineering:**
- Better comments and organized steps
- Improved SQL organization
- Better structure but still monolithic
- Reduced code duplications (average 4.5%)

**Specification-Driven Development:**
- Extracted functions (DRY principle)
- Separated validation functions
- Repository pattern for data access
- Service layer for business logic
- Low code duplications (best: 0.0%)

## Critical Issues Identified

### 1. Concurrency Safety

**Basic Prompting:**
- ‚ùå Race conditions in stock management
- ‚ùå Non-atomic operations (low stock alerts outside transactions)
- ‚ö†Ô∏è Basic row locking (FOR UPDATE) but incomplete implementation

**Context Engineering:**
- ‚úÖ Improved atomicity (low stock alerts inside transactions)
- ‚úÖ Better overselling prevention
- ‚úÖ Proper boundary checks

**Specification-Driven Development:**
- ‚úÖ Complete atomicity through transaction helpers
- ‚úÖ Repository pattern ensures consistency
- ‚úÖ Comprehensive concurrency safety

### 2. Code Duplications

**Basic Prompting:**
- ‚ùå Extremely high (worst: 65.5%)
- ‚ùå Copy-paste code patterns
- ‚ùå No extracted functions

**Context Engineering:**
- ‚úÖ Significantly reduced (average: 4.5%, best: 0.0%)
- ‚úÖ Better code organization
- ‚ö†Ô∏è Still some duplication

**Specification-Driven Development:**
- ‚úÖ Low duplications (best: 0.0%)
- ‚úÖ Extracted functions and utilities
- ‚úÖ DRY principle applied

### 3. Security Vulnerabilities

**Basic Prompting:**
- ‚úÖ Generally good (0 vulnerabilities in most implementations)
- ‚ö†Ô∏è Some implementations have security hotspots

**Context Engineering:**
- ‚úÖ Good security (0 vulnerabilities)
- ‚ö†Ô∏è Security hotspots require review

**Specification-Driven Development:**
- ‚úÖ Best implementation achieved 0 vulnerabilities (v.11)
- ‚ö†Ô∏è Some implementations have vulnerabilities: v.9 (1), v.10 (5 - highest across all versions)
- ‚ö†Ô∏è Average security vulnerabilities: 2.0 per SDD implementation
- **Note:** While the best SDD implementation achieves zero vulnerabilities, the method does not guarantee security by itself - one SDD implementation had the highest vulnerability count (5) across all evaluated versions

### 4. Test Coverage

**All Methods:**
- ‚ùå Test Coverage: 0.0% across all implementations
- ‚ö†Ô∏è This is a critical limitation affecting all evaluation methods
- Recommendation: Future work should include comprehensive test coverage

## Conclusions

1. **Specification-Driven Development produces the highest quality code** with Clean Architecture, structured error handling, and comprehensive validation. The best implementation achieved 5‚≠ê quality rating with 0.0% code duplications and zero security vulnerabilities. However, SDD implementations show variability in security outcomes, with one implementation having 5 vulnerabilities (the highest across all versions), indicating that SDD alone does not guarantee security without proper security-focused specifications and review.

2. **Context Engineering significantly improves error handling and reduces code duplications** compared to Basic Prompting. It achieves structured error codes and better transaction atomicity, though it still lacks the architectural benefits of SDD.

3. **Basic Prompting can satisfy functional requirements** but consistently fails under concurrent conditions, exhibiting race conditions and lack of transaction atomicity. It also suffers from high code duplications (up to 65.5%) and poor code organization.

4. **All methods require improvement in test coverage** (currently 0.0% across all implementations), which is critical for production-ready software.

5. **The evolution from Basic Prompting ‚Üí Context Engineering ‚Üí Specification-Driven Development** demonstrates clear improvements in:
   - Architecture (Monolithic ‚Üí Clean Architecture)
   - Error Handling (Basic ‚Üí Structured ‚Üí Error Factory)
   - Code Organization (All-in-one ‚Üí Better structure ‚Üí Extracted Functions)
   - Transaction Safety (Non-atomic ‚Üí Atomic ‚Üí Transaction Helpers)

## Recommendations

### For AI-Assisted Development

1. **Adopt Specification-Driven Development for new projects**
   - Provides best results (Average Quality 4.33‚≠ê)
   - Best implementations achieve 5‚≠ê with Clean Architecture
   - Requires detailed specifications before development

2. **Use Context Engineering to reduce code duplications**
   - Best implementations achieved 0.0% code duplications
   - Provides structured error handling
   - Requires good context about requirements and best practices

3. **Basic Prompting requires careful review**
   - Can produce good features but with high code duplications
   - Requires code review and refactoring after development
   - Not recommended for production without extensive review

### For Production Systems

1. **Prioritize transaction atomicity** - All critical operations must be within transactions
2. **Implement structured error handling** - Use Error Factory pattern or structured error codes
3. **Apply Clean Architecture** - Repository Pattern, Service Layer, separated concerns
4. **Reduce code duplications** - Extract common functions, apply DRY principle
5. **Achieve comprehensive test coverage** - Target 70-80% coverage
6. **Address security vulnerabilities** - Zero tolerance for security issues

## Keywords

AI Code Generation, Specification-Driven Development, Context Engineering, Software Quality, Large Language Models, Clean Architecture, Code Quality Metrics, Static Analysis

---

## Summary

**Multi-Dimensional Reporting Results:**

**1. Functional Correctness (Test Results):**
- Basic Prompting (v.3, v.4, v.5): All tests passed (100%), including critical edge cases (race conditions, transaction atomicity, coupon validation)
- Context Engineering (v.6, v.7, v.8): All tests passed (100%), including race conditions, transaction atomicity, and coupon validation
- SDD (v.9, v.10, v.11): All tests passed (100%), including complex scenarios (order of operations, negative total protection, transaction helpers)

**2. SonarQube Static Analysis:**
- **Security:** Basic Prompting and Context Engineering: 0 vulnerabilities across all implementations. SDD: Variable (0-5 vulnerabilities, v.10 highest with 5)
- **Reliability:** Best: v.5 (1 issue). Worst: v.9 (22 issues). SDD average: 13 issues per implementation
- **Code Duplications:** Best: v.8 and v.11 (0.0%). Worst: v.4 (65.5%). Context Engineering average: 4.5%, SDD average: 12.6%
- **Critical Issue:** Test Coverage = 0.0% across ALL versions

**3. CodeQL Security Analysis:**
- v.3-v.8: No critical security issues reported (aligns with SonarQube findings)
- v.9-v.11: CodeQL reports not available

**Key Findings:**
- Basic Prompting can satisfy functional requirements (100% test pass rate) but shows high code duplications (up to 65.5%)
- Context Engineering achieves zero security vulnerabilities and significantly reduces code duplications (best: 0.0%)
- SDD shows variability: best implementation (v.11) achieves 0 vulnerabilities and 0% duplications, but worst (v.10) has 5 vulnerabilities
- **Universal Issue:** Test coverage remains 0.0% across all methods, highlighting the need for systematic automated testing

These findings emphasize that structured specification-driven contexts can improve code quality, but security and test coverage must be explicitly addressed in the development process.

---

## Methodology Summary for Research Paper

### Multi-Dimensional Reporting (Primary Outcomes)

This study employs **three independent measurement standards** to assess code quality objectively:

**1. Functional Correctness Evaluation**
- **Method:** Automated acceptance tests using Jest
- **Test Scenarios:** Defined in `scenarios_*.md` files for each version
- **Coverage:** Standard scenarios + edge cases (race conditions, transaction atomicity, boundary values)
- **Metrics:** Pass/fail rates, test count, edge case performance

**2. SonarQube Static Analysis**
- **Method:** Automated static code analysis
- **Metrics:**
  - Security Vulnerabilities (count)
  - Reliability Issues / Bugs (count)
  - Maintainability Issues / Code Smells (count)
  - Security Hotspots (count)
  - Test Coverage (percentage)
  - Code Duplications (percentage)
- **Lines of Code:** 9,285 total across all versions

**3. CodeQL Security Analysis**
- **Method:** Automated security vulnerability detection
- **Metrics:** Critical security issues (presence/absence)
- **Coverage:** v.3-v.8 (v.9-v.11 reports not available)

### Note on Subjective Assessments

**Rubric-Based Quality Score** (if included) should be reported as a **secondary outcome** with explicit acknowledgment of:
- Single-rater assessment (no inter-rater reliability)
- Architecture-centric focus (may not align with quantitative metrics)
- Retrospective application

**Scoring Process:**
- Single expert reviewer using predefined rubric
- Systematic application of rubric criteria (4 dimensions √ó 0-3 points)
- Retrospective application to ensure consistency
- Documentation of scoring rationale based on observable code characteristics

**Important Limitations:**
- ‚ö†Ô∏è **Single Rater**: No inter-rater reliability measures were calculated (Cohen's Œ∫, ICC not applicable)
- ‚ö†Ô∏è **No Test-Retest**: Test-retest reliability was not assessed
- ‚ö†Ô∏è **Potential Bias**: Single-rater assessment may introduce personal bias
- ‚ö†Ô∏è **Retrospective Application**: Rubric developed after initial assessments

**Mitigation Strategies:**
- Rubric-based scoring reduces subjectivity compared to holistic ratings
- Transparent, observable criteria (see Table 1)
- Reproducible scoring process

**Scope and Limitations:**
- Quality Score focuses on architecture-centric assessment and best practices
- It may not always align with quantitative metrics (e.g., code duplications, security vulnerabilities)
- Quality Score is a secondary outcome and should be interpreted alongside primary quantitative metrics
- The rubric emphasizes structural quality over bug counts or security issues
- **Single-rater assessment limits generalizability** - results should be interpreted with caution

**For Paper Appendix:**
- Include Table 1 (Quality Score Rubric) in appendix
- **Explicitly acknowledge single-rater limitation**
- Acknowledge that inter-rater reliability was not assessed
- Clarify that Quality Score is architecture-centric and may differ from quantitative metrics
- Recommend future work with multiple raters and reliability measures

---

*This evaluation demonstrates the necessity of structured, specification-driven contexts for generating reproducible and production-ready software with large language models, while highlighting that security considerations must be explicitly incorporated into the development process. The rubric-based assessment provides a systematic, reproducible method for evaluating structural quality, complementing quantitative metrics from automated analysis tools.*


‡∏Ç‡πâ‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏•‡πà‡∏°‡∏ß‡∏¥‡∏à‡∏±‡∏¢:
Section III (Methodology): ‡πÉ‡∏´‡πâ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ "Multi-Dimensional Reporting" ‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ 3 ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¥‡∏™‡∏£‡∏∞‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏≥‡πÄ‡∏≠‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏î‡∏¢‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå

Section IV (Results): ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏£‡∏≤‡∏á "Results by Version" ‡∏à‡∏≤‡∏Å SonarQube ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å (Primary Table) ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏´‡∏ô‡∏±‡∏Å‡πÅ‡∏ô‡πà‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î

Section V (Discussion): ‡πÉ‡∏´‡πâ‡∏≠‡∏†‡∏¥‡∏õ‡∏£‡∏≤‡∏¢‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á "Test Coverage 0.0%" ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏±‡∏á ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ ‡∏ã‡∏∂‡πà‡∏á‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏á‡∏Å‡∏≤‡∏£ AI Code Generation